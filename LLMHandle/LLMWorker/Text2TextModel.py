# -*- coding: utf-8 -*-
import os
import re
import requests
from abc import ABC, abstractmethod
from openai import OpenAI
from typing import Generator, Tuple, Deque  # ä¸ºç±»å‹æç¤ºæ·»åŠ  Deque
from collections import deque  # å¯¼å…¥ deque
import threading  # å¯¼å…¥ threading

# å¥½çš„åšæ³•æ˜¯å¯¼å…¥æ‚¨æ‰“ç®—æ•è·çš„ç‰¹å®šå¼‚å¸¸ã€‚
# ç¡®ä¿æ‚¨çš„ openai åº“ç‰ˆæœ¬æ˜¯æœ€æ–°çš„ï¼Œä»¥ä¾¿åŒ…å«è¿™äº›å¼‚å¸¸ã€‚
try:
    from openai import APIConnectionError, AuthenticationError, RateLimitError, APIStatusError, NotFoundError
except ImportError:
    # å¦‚æœ openai åº“ç‰ˆæœ¬è¿‡æ—§æˆ–æœªå®Œå…¨å®‰è£…ï¼Œåˆ™å®šä¹‰è™šæ‹Ÿå¼‚å¸¸
    class APIConnectionError(Exception):
        pass


    class AuthenticationError(Exception):
        pass


    class RateLimitError(Exception):
        pass


    class APIStatusError(Exception):
        pass


    class NotFoundError(Exception):
        pass

# å‡è®¾ config å’Œ PromptLoader åœ¨è¿™äº›ä½ç½®
from LLMDispatch.LLMHandle import config
from LLMDispatch.LLMHandle.LLMWorker.PromptLoader import load_prompt
from dataclasses import dataclass
from typing import Literal


@dataclass
class DeepThinkChunk:
    """å°è£…æ·±åº¦æ€è€ƒæµä¸­çš„ä¸€ä¸ªæ•°æ®å—åŠå…¶ç±»å‹ã€‚"""
    type: Literal['reasoning', 'response']
    content: str


class LLMStreamSplitter:
    def __init__(self, client_create_method, model_id: str, messages: list, temperature: float, **kwargs):
        self._client_create_method = client_create_method  # OpenAI client.chat.completions.create
        self._model_id = model_id
        self._messages = messages
        self._temperature = temperature
        self._kwargs = kwargs

        self._stream_iter: Generator | None = None
        self._reasoning_buffer: Deque[str] = deque()
        self._response_buffer: Deque[str] = deque()

        self._stream_initialized_flag = False
        self._stream_exhausted_flag = False

        self._lock = threading.Lock()

    def _initialize_stream_if_needed(self):
        if not self._stream_initialized_flag:
            try:
                stream = self._client_create_method(
                    model=self._model_id,
                    messages=self._messages,
                    temperature=self._temperature,
                    stream=True,
                    **self._kwargs
                )
                self._stream_iter = iter(stream)
                self._stream_initialized_flag = True
            except Exception as e:
                print(f"é”™è¯¯ï¼šåˆå§‹åŒ–APIæµå¤±è´¥ (LLMStreamSplitter): {e}")
                self._stream_exhausted_flag = True

    def _fetch_next_chunk_into_buffers(self) -> bool:
        with self._lock:
            if not self._stream_initialized_flag:
                self._initialize_stream_if_needed()
                if not self._stream_initialized_flag:
                    return False

            if self._stream_exhausted_flag:
                return False

            try:
                if self._stream_iter is None:
                    self._stream_exhausted_flag = True
                    return False
                chunk = next(self._stream_iter)

                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta:
                        # å…¼å®¹ qwen çš„ enable_thinking è¿”å›
                        if hasattr(delta, 'tool_calls') and delta.tool_calls:
                            for tool_call in delta.tool_calls:
                                if tool_call.function and tool_call.function.name == "thought":
                                    self._reasoning_buffer.append(tool_call.function.arguments)
                        # å…¼å®¹ deepseek çš„ reasoning_content
                        elif hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                            self._reasoning_buffer.append(delta.reasoning_content)
                        # é€šç”¨å†…å®¹
                        if delta.content:
                            self._response_buffer.append(delta.content)

                    if chunk.choices[0].finish_reason:
                        self._stream_exhausted_flag = True
                return True

            except StopIteration:
                self._stream_exhausted_flag = True
                return False
            except Exception as e:
                print(f"é”™è¯¯ï¼šä»æµä¸­è·å–æ•°æ®æ—¶å‡ºé”™ (LLMStreamSplitter): {e}")
                self._stream_exhausted_flag = True
                return False

    def stream(self) -> Generator[DeepThinkChunk, None, None]:
        """
        ç”Ÿæˆä¸€ä¸ªç»Ÿä¸€çš„æµï¼ŒåŒ…å«æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå“åº”ã€‚
        """
        while True:
            chunk_yielded = False

            # å…ˆæ¸…ç©ºç¼“å†²åŒºä¸­çš„å†…å®¹
            with self._lock:
                if self._reasoning_buffer:
                    yield DeepThinkChunk(type='reasoning', content=self._reasoning_buffer.popleft())
                    chunk_yielded = True
                elif self._response_buffer:
                    yield DeepThinkChunk(type='response', content=self._response_buffer.popleft())
                    chunk_yielded = True

            # å¦‚æœæœ¬æ¬¡å¾ªç¯äº§å‡ºäº†å†…å®¹ï¼Œåˆ™ç«‹å³å¼€å§‹ä¸‹ä¸€æ¬¡å¾ªç¯ï¼Œä»¥å°½å¿«æ¸…ç©ºç¼“å†²åŒº
            if chunk_yielded:
                continue

            # å¦‚æœç¼“å†²åŒºä¸ºç©ºï¼Œå°è¯•ä»ä¸Šæ¸¸è·å–æ–°æ•°æ®
            # å¦‚æœè·å–å¤±è´¥æˆ–æµå·²ç»“æŸï¼Œåˆ™è·³å‡ºå¾ªç¯
            if not self._fetch_next_chunk_into_buffers():
                # åœ¨é€€å‡ºå‰ï¼Œå†æ¬¡æ£€æŸ¥å¹¶æ¸…ç©ºå¯èƒ½åœ¨æœ€åä¸€æ¬¡ fetch ä¸­å¡«å……çš„ç¼“å†²åŒº
                with self._lock:
                    while self._reasoning_buffer:
                        yield DeepThinkChunk(type='reasoning', content=self._reasoning_buffer.popleft())
                    while self._response_buffer:
                        yield DeepThinkChunk(type='response', content=self._response_buffer.popleft())
                break  # ç»“æŸç”Ÿæˆå™¨


# --- æŠ½è±¡åŸºç±» ---
@dataclass
class BaseTextModelAPI(ABC):
    temperature: float = 0.7  # é»˜è®¤æ¸©åº¦

    @abstractmethod
    def generate_text(self, query: str, temperature: float = None, **kwargs) -> str:
        pass

    @abstractmethod
    def deepthink_generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[
        DeepThinkChunk, None, None]:
        """
        ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼Œå°†æ€ç»´è¿‡ç¨‹å’Œæœ€ç»ˆç»“æœåˆå¹¶åˆ°å•ä¸ªæµä¸­ã€‚
        è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œäº§å‡º DeepThinkChunk å¯¹è±¡ã€‚
        """
        pass

    @abstractmethod
    def generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[str, None, None]:
        """ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼Œé€å—äº§ç”Ÿå†…å®¹ã€‚"""
        pass

    @abstractmethod
    def change_temperature(self, temperature: float):
        self.temperature = temperature

    @abstractmethod
    def set_custom_prompt(self, user_prompt: str, mode: str = "append"):
        pass

    @abstractmethod
    def get_prompt(self) -> dict:
        pass

    @abstractmethod
    def reload_prompt(self, role: str):
        pass

    @abstractmethod
    def api_test(self) -> bool:
        pass


# --- è®¯é£ API å°è£… ---
class XunfeiTextModelAPI(BaseTextModelAPI):
    def __init__(self,
                 api_key: str = None,
                 base_url: str = "https://spark-api-open.xf-yun.com/v2",  # ä½¿ç”¨å…¼å®¹OpenAIçš„V2åœ°å€
                 model: str = "x1",  # V2æ¥å£ä½¿ç”¨çš„æ¨¡å‹æ ‡è¯†ç¬¦ä¸º "x1"
                 role: str = "default",
                 temperature: float = 0.5):
        super().__init__(temperature=temperature)
        # æ³¨æ„ï¼šè®¯é£V2æ¥å£çš„API Keyæ ¼å¼ä¸º "APIKey:APISecret" æˆ–ç›´æ¥ä½¿ç”¨æ§åˆ¶å°çš„ "APIPassword"
        self.api_key = api_key if api_key is not None else config.XUNFEI_API_KEY
        self.base_url = base_url
        self.model = model
        self.role = role
        self.prompt_config = load_prompt(role)
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def deepthink_generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[DeepThinkChunk, None, None]:
        """
        ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼Œå°†æ€ç»´è¿‡ç¨‹å’Œæœ€ç»ˆç»“æœåˆå¹¶åˆ°å•ä¸ªæµä¸­ã€‚
        è®¯é£X1æ¨¡å‹æ”¯æŒ reasoning_contentï¼Œå¯ç›´æ¥ä½¿ç”¨LLMStreamSplitterã€‚
        """
        current_temp = temperature if temperature is not None else self.temperature
        messages = []
        if self.prompt_config.get("system"):
            messages.append({"role": "system", "content": self.prompt_config["system"]})
        messages.append({"role": "user", "content": self.prompt_config.get("user_prefix", "") + query})

        splitter = LLMStreamSplitter(
            client_create_method=self.client.chat.completions.create,
            model_id=self.model,  # ä½¿ç”¨æ”¯æŒæ€ç»´é“¾çš„x1æ¨¡å‹
            messages=messages,
            temperature=current_temp,
            **kwargs
        )
        return splitter.stream()

    def generate_text(self, query: str, temperature: float = None, **kwargs) -> str:
        """
        ä½¿ç”¨å…¼å®¹OpenAIçš„SDKç”Ÿæˆæ–‡æœ¬ï¼ˆéæµå¼ï¼‰ã€‚
        """
        current_temp = temperature if temperature is not None else self.temperature
        messages = [
            {"role": "system", "content": self.prompt_config.get("system", "")},
            {"role": "user", "content": self.prompt_config.get("user_prefix", "") + query}
        ]
        if not messages[0]["content"]:  # å¦‚æœç³»ç»Ÿæ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œåˆ™ç§»é™¤å®ƒ
            messages.pop(0)

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=current_temp,
                stream=False,
                **kwargs
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"è®¯é£ API è°ƒç”¨å¤±è´¥: {e}")
            return "è®¯é£ API è°ƒç”¨å¤±è´¥"

    def generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[str, None, None]:
        """ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼Œé€å—äº§ç”Ÿå†…å®¹ã€‚"""
        current_temp = temperature if temperature is not None else self.temperature
        messages = [
            {"role": "system", "content": self.prompt_config.get("system", "")},
            {"role": "user", "content": self.prompt_config.get("user_prefix", "") + query}
        ]
        if not messages[0]["content"]:  # å¦‚æœç³»ç»Ÿæ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œåˆ™ç§»é™¤å®ƒ
            messages.pop(0)

        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=current_temp,
                stream=True,
                **kwargs
            )
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            print(f"è®¯é£ API æµå¼è°ƒç”¨å¤±è´¥: {e}")
            # å¯ä»¥é€‰æ‹© yield ä¸€ä¸ªé”™è¯¯ä¿¡æ¯æˆ–ç›´æ¥è¿”å›
            yield f"è®¯é£ API æµå¼è°ƒç”¨å¤±è´¥: {e}"

    def api_test(self) -> bool:
        """
        ä½¿ç”¨å…¼å®¹OpenAIçš„SDKæµ‹è¯•APIè¿é€šæ€§ã€‚
        """
        try:
            self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5,
                stream=False,
                timeout=10.0
            )
            return True
        except (APIConnectionError, AuthenticationError, RateLimitError, NotFoundError, APIStatusError):
            return False
        except Exception:
            return False

    def change_temperature(self, temperature: float):
        self.temperature = temperature

    def set_custom_prompt(self, user_prompt: str, mode: str = "append"):
        if mode == "replace":
            self.prompt_config["user_prefix"] = user_prompt
        elif mode == "append":
            current_prefix = self.prompt_config.get("user_prefix", "")
            self.prompt_config["user_prefix"] = current_prefix + "\n" + user_prompt if current_prefix else user_prompt
        else:
            raise ValueError("mode åªèƒ½æ˜¯ 'replace' æˆ– 'append'")

    def get_prompt(self) -> dict:
        return {
            "system": self.prompt_config.get("system", ""),
            "user_prefix": self.prompt_config.get("user_prefix", "")
        }

    def reload_prompt(self, role: str = None):
        current_role = role if role is not None else self.role
        self.prompt_config = load_prompt(current_role)
        self.role = current_role


# --- DeepSeek å°è£… ---
class DeepseekTextModelAPI(BaseTextModelAPI):
    def __init__(self,
                 api_key: str = None,
                 base_url: str = "https://api.deepseek.com",
                 model: str = "deepseek-chat",
                 role: str = "default",
                 temperature: float = 0.7):
        super().__init__(temperature=temperature)
        self.api_key = api_key if api_key is not None else config.DEEPSEEK_API_KEY
        self.base_url = base_url
        self.model = model
        self.role = role
        self.prompt_config = load_prompt(role)
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def deepthink_generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[DeepThinkChunk, None, None]:
        current_temp = temperature if temperature is not None else self.temperature
        messages = []
        if self.prompt_config.get("system"):
            messages.append({"role": "system", "content": self.prompt_config["system"]})
        messages.append({"role": "user", "content": self.prompt_config.get("user_prefix", "") + query})

        splitter = LLMStreamSplitter(
            client_create_method=self.client.chat.completions.create,
            model_id='deepseek-reasoner',  # ä½¿ç”¨æ”¯æŒæ€ç»´é“¾çš„æ¨¡å‹
            messages=messages,
            temperature=current_temp,
            **kwargs
        )
        return splitter.stream()

    def generate_text(self, query: str, temperature: float = None, **kwargs) -> str:
        current_temp = temperature if temperature is not None else self.temperature
        messages = [
            {"role": "system", "content": self.prompt_config.get("system", "")},
            {"role": "user", "content": self.prompt_config.get("user_prefix", "") + query}
        ]
        if not messages[0]["content"]:  # å¦‚æœç³»ç»Ÿæ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œåˆ™ç§»é™¤å®ƒ
            messages.pop(0)

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=current_temp,
            stream=False,
            **kwargs
        )
        return response.choices[0].message.content.strip()

    def generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[str, None, None]:
        current_temp = temperature if temperature is not None else self.temperature
        messages = [
            {"role": "system", "content": self.prompt_config.get("system", "")},
            {"role": "user", "content": self.prompt_config.get("user_prefix", "") + query}
        ]
        if not messages[0]["content"]:  # å¦‚æœç³»ç»Ÿæ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œåˆ™ç§»é™¤å®ƒ
            messages.pop(0)

        stream = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=current_temp,
            stream=True,
            **kwargs
        )
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def api_test(self) -> bool:
        try:
            test_client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            test_client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5,
                stream=False,
                timeout=10.0
            )
            return True
        except (APIConnectionError, AuthenticationError, RateLimitError, NotFoundError, APIStatusError):
            return False
        except Exception:
            return False

    def change_temperature(self, temperature: float):
        self.temperature = temperature

    def set_custom_prompt(self, user_prompt: str, mode: str = "append"):
        if mode == "replace":
            self.prompt_config["user_prefix"] = user_prompt
        elif mode == "append":
            current_prefix = self.prompt_config.get("user_prefix", "")
            self.prompt_config["user_prefix"] = current_prefix + "\n" + user_prompt if current_prefix else user_prompt
        else:
            raise ValueError("mode åªèƒ½æ˜¯ 'replace' æˆ– 'append'")

    def get_prompt(self) -> dict:
        return {
            "system": self.prompt_config.get("system", ""),
            "user_prefix": self.prompt_config.get("user_prefix", "")
        }

    def reload_prompt(self, role: str = None):
        current_role = role if role is not None else self.role
        self.prompt_config = load_prompt(current_role)
        self.role = current_role


# --- Qwen å°è£… ---
class QwenTextModelAPI(BaseTextModelAPI):
    def __init__(self,
                 api_key: str = None,
                 base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                 model: str = "qwen3-235b-a22b",  # ä¾‹å¦‚: "qwen-turbo", "qwen-plus", "qwen-max"
                 role: str = "default",
                 temperature: float = 0.7):  # Qwen é»˜è®¤æ¸©åº¦æ˜¯ 1.0
        super().__init__(temperature=temperature)
        self.api_key = api_key if api_key is not None else config.QWEN_API_KEY
        self.base_url = base_url
        self.model = model
        self.role = role
        self.prompt_config = load_prompt(role)
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def deepthink_generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[DeepThinkChunk, None, None]:
        current_temp = temperature if temperature is not None else self.temperature
        messages = []
        if self.prompt_config.get("system"):
            messages.append({"role": "system", "content": self.prompt_config["system"]})
        messages.append({"role": "user", "content": self.prompt_config.get("user_prefix", "") + query})

        splitter = LLMStreamSplitter(
            client_create_method=self.client.chat.completions.create,
            model_id=self.model,  # ä½¿ç”¨æ”¯æŒæ€ç»´é“¾çš„æ¨¡å‹
            messages=messages,
            temperature=current_temp,
            extra_body={"enable_thinking": True},
            **kwargs
        )
        return splitter.stream()

    def generate_text(self, query: str, temperature: float = None, **kwargs) -> str:
        current_temp = temperature if temperature is not None else self.temperature
        messages = [
            {"role": "system", "content": self.prompt_config.get("system", "")},
            {"role": "user", "content": self.prompt_config.get("user_prefix", "") + query}
        ]
        if not messages[0]["content"]:  # å¦‚æœç³»ç»Ÿæ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œåˆ™ç§»é™¤å®ƒ
            messages.pop(0)

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=current_temp,
            stream=False,
            extra_body={"enable_thinking": False},
            **kwargs
        )
        return response.choices[0].message.content.strip()

    def generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[str, None, None]:
        current_temp = temperature if temperature is not None else self.temperature
        messages = [
            {"role": "system", "content": self.prompt_config.get("system", "")},
            {"role": "user", "content": self.prompt_config.get("user_prefix", "") + query}
        ]
        if not messages[0]["content"]:  # å¦‚æœç³»ç»Ÿæ¶ˆæ¯å†…å®¹ä¸ºç©ºï¼Œåˆ™ç§»é™¤å®ƒ
            messages.pop(0)

        stream = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=current_temp,
            stream=True,
            extra_body={"enable_thinking": False},
            **kwargs
        )
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def api_test(self) -> bool:
        try:
            test_client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            test_client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5,
                stream=False,
                extra_body={"enable_thinking": False},
                timeout=10.0
            )
            return True
        except (APIConnectionError, AuthenticationError, RateLimitError, NotFoundError, APIStatusError):
            return False
        except Exception:
            return False

    def change_temperature(self, temperature: float):
        self.temperature = temperature

    def set_custom_prompt(self, user_prompt: str, mode: str = "append"):
        if mode == "replace":
            self.prompt_config["user_prefix"] = user_prompt
        elif mode == "append":
            current_prefix = self.prompt_config.get("user_prefix", "")
            self.prompt_config["user_prefix"] = current_prefix + "\n" + user_prompt if current_prefix else user_prompt
        else:
            raise ValueError("mode åªèƒ½æ˜¯ 'replace' æˆ– 'append'")

    def get_prompt(self) -> dict:
        return {
            "system": self.prompt_config.get("system", ""),
            "user_prefix": self.prompt_config.get("user_prefix", "")
        }

    def reload_prompt(self, role: str = None):
        current_role = role if role is not None else self.role
        self.prompt_config = load_prompt(current_role)
        self.role = current_role


# --- è±†åŒ…å°è£… ---
class DoubaoTextModelAPI(BaseTextModelAPI):
    def __init__(self,
                 api_key: str = None,
                 base_url: str = "https://ark.cn-beijing.volces.com/api/v3",  # ç¡®ä¿è¿™æ˜¯ OpenAI å…¼å®¹çš„ç«¯ç‚¹
                 model: str = "doubao-1-5-pro-32k-250115",
                 role: str = "default",
                 temperature: float = 0.7):
        super().__init__(temperature=temperature)
        self.api_key = api_key if api_key is not None else config.DOUBAO_API_KEY
        self.base_url = base_url
        self.model = model  # å¯¹äºä½¿ç”¨ OpenAI å…¼å®¹ API çš„è±†åŒ…ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ª endpoint_id
        self.role = role
        self.prompt_config = load_prompt(role)
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def deepthink_generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[DeepThinkChunk, None, None]:
        current_temp = temperature if temperature is not None else self.temperature
        messages = []
        if self.prompt_config.get("system"):
            messages.append({"role": "system", "content": self.prompt_config["system"]})
        messages.append({"role": "user", "content": self.prompt_config.get("user_prefix", "") + query})

        # print("DEBUG: Doubao deepthink - Preparing to create LLMStreamSplitter") # ç”¨äºè°ƒè¯•
        splitter = LLMStreamSplitter(
            client_create_method=self.client.chat.completions.create,
            model_id="doubao-1-5-thinking-pro-m-250428",  # ä½¿ç”¨æ”¯æŒæ€ç»´é“¾çš„æ¨¡å‹
            messages=messages,
            temperature=current_temp,
            **kwargs
        )
        # print("DEBUG: Doubao deepthink - LLMStreamSplitter created, returning generators") # ç”¨äºè°ƒè¯•
        return splitter.stream()

    def generate_text(self, query: str, temperature: float = None, **kwargs) -> str:
        current_temp = temperature if temperature is not None else self.temperature
        messages = []
        if self.prompt_config.get("system"):  # è±†åŒ…å¦‚æœæä¾›äº†ç³»ç»Ÿæç¤ºï¼Œåˆ™å€¾å‘äºä½¿ç”¨å®ƒ
            messages.append({"role": "system", "content": self.prompt_config["system"]})
        messages.append({"role": "user", "content": self.prompt_config.get("user_prefix", "") + query})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=current_temp,
            stream=False,
            **kwargs
        )
        return response.choices[0].message.content.strip()

    def generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[str, None, None]:
        current_temp = temperature if temperature is not None else self.temperature
        messages = []
        if self.prompt_config.get("system"):
            messages.append({"role": "system", "content": self.prompt_config["system"]})
        messages.append({"role": "user", "content": self.prompt_config.get("user_prefix", "") + query})

        stream = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=current_temp,
            stream=True,
            **kwargs
        )
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def api_test(self) -> bool:
        try:
            self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5,
                stream=False,
                timeout=10.0
            )
            return True
        except (APIConnectionError, AuthenticationError, RateLimitError, NotFoundError, APIStatusError) as e:
            # print(f"è±†åŒ… API æµ‹è¯•å¤±è´¥ (OpenAI ç‰¹å®šé”™è¯¯): {e}") # å¯é€‰: ç”¨äºè°ƒè¯•
            return False
        except Exception as e:
            # print(f"è±†åŒ… API æµ‹è¯•å¤±è´¥ (é€šç”¨é”™è¯¯): {e}") # å¯é€‰: ç”¨äºè°ƒè¯•
            return False

    def change_temperature(self, temperature: float):
        self.temperature = temperature

    def set_custom_prompt(self, user_prompt: str, mode: str = "append"):
        if mode == "replace":
            self.prompt_config["user_prefix"] = user_prompt
        elif mode == "append":
            current_prefix = self.prompt_config.get("user_prefix", "")
            self.prompt_config["user_prefix"] = current_prefix + "\n" + user_prompt if current_prefix else user_prompt
        else:
            raise ValueError("mode åªèƒ½æ˜¯ 'replace' æˆ– 'append'")

    def get_prompt(self) -> dict:
        return {
            "system": self.prompt_config.get("system", ""),
            "user_prefix": self.prompt_config.get("user_prefix", "")
        }

    def reload_prompt(self, role: str = None):
        current_role = role if role is not None else self.role
        self.prompt_config = load_prompt(current_role)
        self.role = current_role


# --- æ€»è°ƒåº¦ç±» ---
class TextGenerationManager:
    _registry = {
        "xunfei": XunfeiTextModelAPI,
        "deepseek": DeepseekTextModelAPI,
        "qwen": QwenTextModelAPI,
        "doubao": DoubaoTextModelAPI,
    }

    def __init__(self, use_api: str = "deepseek", role: str = "default", temperature: float = None):
        if use_api not in self._registry:
            raise ValueError(f"ä¸æ”¯æŒçš„ API: {use_api}. å¯ç”¨: {list(self._registry.keys())}")
        self.use_api = use_api
        init_kwargs = {'role': role}
        if temperature is not None:
            init_kwargs['temperature'] = temperature

        # åˆå§‹åŒ–å‰ç¡®ä¿ API å¯†é’¥å¯ç”¨
        api_name_upper = use_api.upper()
        config_key_name = f"{api_name_upper}_API_KEY"
        if not getattr(config, config_key_name, None) and not init_kwargs.get('api_key'):
            print(f"è­¦å‘Š: æœªåœ¨é…ç½®ä¸­æ‰¾åˆ° {use_api} çš„ API å¯†é’¥ ({config_key_name})ï¼Œä¹Ÿæœªç›´æ¥æä¾›ã€‚åˆå§‹åŒ–å¯èƒ½ä¼šå¤±è´¥ã€‚")

        self.client: BaseTextModelAPI = self._registry[use_api](**init_kwargs)

    def deepthink_generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[DeepThinkChunk, None, None]:
        return self.client.deepthink_generate_text_stream(query, temperature=temperature, **kwargs)

    def generate_text(self, query: str, temperature: float = None, **kwargs) -> str:
        return self.client.generate_text(query, temperature=temperature, **kwargs)

    def generate_text_stream(self, query: str, temperature: float = None, **kwargs) -> Generator[str, None, None]:
        """ä»å½“å‰é€‰å®šçš„ API å®¢æˆ·ç«¯ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬ã€‚"""
        return self.client.generate_text_stream(query, temperature=temperature, **kwargs)

    def change_temperature(self, temperature: float):
        self.client.change_temperature(temperature)

    def set_custom_prompt(self, user_prompt: str, mode: str = "append"):
        self.client.set_custom_prompt(user_prompt, mode)

    def get_prompt(self) -> dict:
        return self.client.get_prompt()

    def reload_prompt(self, role: str = None):
        self.client.reload_prompt(role)

    def switch_api(self, use_api: str, role: str = None, temperature: float = None, api_key: str = None):
        if use_api not in self._registry:
            raise ValueError(f"ä¸æ”¯æŒçš„ API: {use_api}. å¯ç”¨: {list(self._registry.keys())}")

        self.use_api = use_api
        current_role = role if role is not None else self.client.role
        current_temp = temperature if temperature is not None else self.client.temperature

        init_kwargs = {'role': current_role, 'temperature': current_temp}
        if api_key:  # å…è®¸åœ¨åˆ‡æ¢æ—¶è¦†ç›– API å¯†é’¥
            init_kwargs['api_key'] = api_key
        else:  # å¦‚æœæœªæä¾›ï¼Œåˆ™æ£€æŸ¥é…ç½®
            api_name_upper = use_api.upper()
            config_key_name = f"{api_name_upper}_API_KEY"
            if not getattr(config, config_key_name, None):
                print(
                    f"è­¦å‘Š: æ­£åœ¨åˆ‡æ¢åˆ° {use_api}ã€‚æœªåœ¨é…ç½®ä¸­æ‰¾åˆ° API å¯†é’¥ ({config_key_name})ï¼Œä¹Ÿæœªç›´æ¥æä¾›ã€‚åˆå§‹åŒ–å¯èƒ½ä¼šå¤±è´¥ã€‚")

        self.client = self._registry[use_api](**init_kwargs)
        print(f"å·²åˆ‡æ¢åˆ° API: {use_api}ï¼Œè§’è‰²: {self.client.role}ï¼Œæ¸©åº¦: {self.client.temperature}")

    def get_current_config(self) -> dict:
        return {
            "api": self.use_api,
            "role": self.client.role,
            "temperature": self.client.temperature,
            "model": getattr(self.client, 'model', 'N/A'),
            "prompts": self.get_prompt()
        }

    def api_test(self) -> bool:
        """æµ‹è¯•æŒ‡å®šçš„ APIï¼Œå¦‚æœæœªæŒ‡å®šï¼Œåˆ™æµ‹è¯•å½“å‰ APIã€‚"""
        print(f"æ­£åœ¨æµ‹è¯•å½“å‰ API: {self.use_api}...")
        is_ok = self.client.api_test()
        print(f"å½“å‰ API ({self.use_api}) æµ‹è¯•ç»“æœ: {'æˆåŠŸ' if is_ok else 'å¤±è´¥'}")
        return is_ok


# ç¤ºä¾‹ç”¨æ³• (è¯´æ˜æ€§ - å‡è®¾ config å’Œ PromptLoader å·²è®¾ç½®)
if __name__ == '__main__':
    import time

    use_api = "xunfei"  # deepseek qwen doubao xunfei

    print("=" * 60)
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ({use_api}) API åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    try:
        manager = TextGenerationManager(use_api=use_api, role="default")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ– TextGenerationManager å¤±è´¥: {e}")
        exit()  # å¦‚æœç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œåˆ™æ— æ³•ç»§ç»­

    # --- 1. API è¿é€šæ€§æµ‹è¯• ---
    print("\n--- [1/4] API è¿é€šæ€§æµ‹è¯• ---")
    if not manager.api_test():
        print("âŒ API è¿é€šæ€§æµ‹è¯•å¤±è´¥ã€‚è¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥å’Œ API å¯†é’¥ã€‚")
        exit()  # å¦‚æœAPIä¸é€šï¼Œåˆ™é€€å‡º

    # å®šä¹‰ä¸€ä¸ªé€šç”¨çš„æŸ¥è¯¢
    query = "ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Œå¹¶ä¸¾ä¸€ä¸ªç”Ÿæ´»ä¸­çš„ä¾‹å­ã€‚"
    print(f"\nä½¿ç”¨é€šç”¨æŸ¥è¯¢è¿›è¡Œåç»­æµ‹è¯•: '{query}'")

    # --- 2. æ ‡å‡†æ–‡æœ¬ç”Ÿæˆæµ‹è¯• (generate_text) ---
    print("\n\n" + "=" * 20 + " [2/4] æ ‡å‡†ç”Ÿæˆæµ‹è¯• " + "=" * 20)
    try:
        start_time = time.time()
        response = manager.generate_text(query)
        end_time = time.time()
        print(f"âœ… æ ‡å‡†ç”ŸæˆæˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}s)")
        print("-" * 50)
        print("å®Œæ•´å›å¤:\n" + response)
        print("-" * 50)
    except Exception as e:
        print(f"âŒ æ ‡å‡†ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")

    # --- 3. æµå¼ç”Ÿæˆæµ‹è¯• (generate_text_stream) ---
    print("\n\n" + "=" * 20 + " [3/4] æ™®é€šæµå¼ç”Ÿæˆæµ‹è¯• " + "=" * 20)
    try:
        print("æµå¼å›å¤:")
        start_time = time.time()
        full_response_stream = ""
        stream_generator = manager.generate_text_stream(query)
        for chunk in stream_generator:
            print(chunk, end="", flush=True)
            full_response_stream += chunk
        end_time = time.time()

        if not full_response_stream.strip():
            print("\nâŒ æ™®é€šæµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: æœªæ”¶åˆ°ä»»ä½•å†…å®¹ã€‚")
        else:
            print(f"\n\nâœ… æ™®é€šæµå¼ç”ŸæˆæˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}s)")

    except Exception as e:
        print(f"\nâŒ æ™®é€šæµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")

    # --- 4. æ·±åº¦æ€è€ƒæµå¼ç”Ÿæˆæµ‹è¯• (deepthink_generate_text_stream) ---
    print("\n\n" + "=" * 20 + " [4/4] æ·±åº¦æ€è€ƒæµå¼ç”Ÿæˆæµ‹è¯• " + "=" * 20)
    # æ¢ä¸€ä¸ªæ›´é€‚åˆæ€è€ƒçš„é—®é¢˜
    deep_query = "å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œä»–åˆå»å•†åº—ä¹°äº†3ç¯®è‹¹æœï¼Œæ¯ç¯®æœ‰4ä¸ªã€‚è¯·é—®ä»–ç°åœ¨ä¸€å…±æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿè¯·å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ã€‚"
    print(f"ä½¿ç”¨æ·±åº¦æ€è€ƒæŸ¥è¯¢: '{deep_query}'")

    try:
        print("æ·±åº¦æ€è€ƒè¿‡ç¨‹æµå¼å›å¤:")
        start_time = time.time()
        full_reasoning = ""
        full_response_deepthink = ""

        deepthink_generator = manager.deepthink_generate_text_stream(deep_query)
        for chunk in deepthink_generator:
            if chunk.type == 'reasoning':
                # ç”¨è“è‰²æ‰“å°æ€è€ƒè¿‡ç¨‹ï¼Œä½¿å…¶æ˜“äºåŒºåˆ†
                print(f"\033[94m{chunk.content}\033[0m", end="", flush=True)
                full_reasoning += chunk.content
            else:
                if not full_response_deepthink:
                    print("\næ·±åº¦æ€è€ƒç»“æœæµå¼å›å¤:")
                print(chunk.content, end="", flush=True)
                full_response_deepthink += chunk.content
        end_time = time.time()

        if not full_reasoning.strip() and not full_response_deepthink.strip():
            print("\nâŒ æ·±åº¦æ€è€ƒæµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: æœªæ”¶åˆ°ä»»ä½•å†…å®¹ã€‚")
        elif not full_reasoning.strip():
            print("\n\nâš ï¸  æ·±åº¦æ€è€ƒæµå¼ç”Ÿæˆè­¦å‘Š: æœªæ”¶åˆ°'æ€è€ƒ'éƒ¨åˆ†ï¼Œæ¨¡å‹å¯èƒ½æœªå¼€å¯æˆ–æœªè§¦å‘æ€è€ƒæ¨¡å¼ã€‚")
            print(f"âœ… æ·±åº¦æ€è€ƒæµå¼ç”Ÿæˆå®Œæˆ (è€—æ—¶: {end_time - start_time:.2f}s)")
        else:
            print(f"\n\nâœ… æ·±åº¦æ€è€ƒæµå¼ç”ŸæˆæˆåŠŸ (è€—æ—¶: {end_time - start_time:.2f}s)")

    except Exception as e:
        print(f"\nâŒ æ·±åº¦æ€è€ƒæµå¼ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")

    print("\n\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæ¯•ã€‚")
    print("=" * 60)




